{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Deep Neural Network for Burst Time Classification\n",
    "class BurstTimeClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BurstTimeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Deep Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Job Scheduling Environment\n",
    "class JobSchedulingEnv:\n",
    "    def __init__(self, k, N):\n",
    "        self.k = k  # number of resource types\n",
    "        self.N = N  # maximum burst time\n",
    "        self.resources = np.zeros(k)\n",
    "        self.backlog = [[] for _ in range(N)]\n",
    "        self.current_time = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.resources = np.zeros(self.k)\n",
    "        self.backlog = [[] for _ in range(self.N)]\n",
    "        self.current_time = 0\n",
    "        return self._get_state()\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        if self.backlog[action]:\n",
    "            job = self.backlog[action].pop(0)\n",
    "            if np.all(self.resources + job['resources'] <= 1):\n",
    "                self.resources += job['resources']\n",
    "                completion_time = self.current_time + action + 1\n",
    "                slowdown = completion_time / job['burst_time']\n",
    "                reward = -slowdown\n",
    "            else:\n",
    "                self.backlog[action].append(job)\n",
    "                reward = -10  # Penalty for invalid action\n",
    "        else:\n",
    "            reward = -10  # Penalty for selecting empty backlog\n",
    "        \n",
    "        self.current_time += 1\n",
    "        if self.current_time >= 1000 or all(len(b) == 0 for b in self.backlog):\n",
    "            done = True\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "        \n",
    "    def _get_state(self):\n",
    "        state = np.concatenate([self.resources] + [np.array([len(b)]) for b in self.backlog])\n",
    "        return state\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 1.0  # discount factor\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0001\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        act_values = self.model(state)\n",
    "        return np.argmax(act_values.cpu().data.numpy())\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_model(next_state).cpu().data.numpy())\n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            loss = nn.MSELoss()(self.model(state), target_f)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Function to generate job signatures\n",
    "def generate_job_signature():\n",
    "    # This is a placeholder. In reality, you would generate this from the ELF file\n",
    "    return np.random.rand(10)  # Assuming 10 features in the job signature\n",
    "\n",
    "# Function to train the Burst Time Classifier\n",
    "def train_burst_time_classifier(classifier, num_samples=100000):\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        job_signature = generate_job_signature()\n",
    "        burst_time = np.random.randint(1, 11)  # Assuming 10 burst time classes\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(torch.FloatTensor(job_signature))\n",
    "        loss = criterion(outputs.unsqueeze(0), torch.LongTensor([burst_time-1]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "def train():\n",
    "    k = 2  # number of resource types\n",
    "    N = 10  # maximum burst time\n",
    "    \n",
    "    env = JobSchedulingEnv(k, N)\n",
    "    state_size = k + N\n",
    "    action_size = N\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    burst_time_classifier = BurstTimeClassifier(10, 100, N)  # Assuming 10 features in job signature\n",
    "    train_burst_time_classifier(burst_time_classifier)\n",
    "    \n",
    "    batch_size = 32\n",
    "    episodes = 1000\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "                break\n",
    "    \n",
    "    torch.save(agent.model.state_dict(), \"job_scheduler_dqn.pth\")\n",
    "    torch.save(burst_time_classifier.state_dict(), \"burst_time_classifier.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
