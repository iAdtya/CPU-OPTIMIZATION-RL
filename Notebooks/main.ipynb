{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "class CPUSchedulerEnv(gym.Env):\n",
    "    def __init__(self, jobs_data):\n",
    "        super(CPUSchedulerEnv, self).__init__()\n",
    "\n",
    "        self.jobs_data = jobs_data\n",
    "        self.num_jobs = len(jobs_data)\n",
    "        self.current_time = 0\n",
    "        self.current_job = None\n",
    "        self.waiting_jobs = []\n",
    "        self.completed_jobs = []\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(\n",
    "            self.num_jobs + 1\n",
    "        )  # +1 for \"do nothing\" action\n",
    "\n",
    "        # Observation space: [current_time, current_job_features, waiting_job_features]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=np.finfo(np.float32).max,\n",
    "            shape=(1 + 5 + 5 * self.num_jobs,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_time = 0\n",
    "        self.current_job = None\n",
    "        self.waiting_jobs = self.jobs_data.copy()\n",
    "        self.completed_jobs = []\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if action == self.num_jobs:  # \"do nothing\" action\n",
    "            self.current_time += 1\n",
    "        elif self.current_job is None and self.waiting_jobs:\n",
    "            self.current_job = self.waiting_jobs.pop(0)\n",
    "        elif self.waiting_jobs:\n",
    "            # Handle preemption if applicable\n",
    "            if self.current_job[\"Preemptive\"] == 1:\n",
    "                self.waiting_jobs.append(self.current_job)\n",
    "                self.current_job = self.waiting_jobs.pop(0)\n",
    "\n",
    "        # Progress current job\n",
    "        if self.current_job is not None:\n",
    "            self.current_job[\"Burst time\"] -= 1\n",
    "            if self.current_job[\"Burst time\"] <= 0:\n",
    "                self.completed_jobs.append(self.current_job)\n",
    "                self.current_job = None\n",
    "\n",
    "        # Calculate reward (example: negative waiting time)\n",
    "        reward = -sum(\n",
    "            [\n",
    "                max(0, self.current_time - job[\"Arrival Time\"])\n",
    "                for job in self.waiting_jobs\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Check if all jobs are completed\n",
    "        if len(self.completed_jobs) == self.num_jobs:\n",
    "            done = True\n",
    "\n",
    "        self.current_time += 1\n",
    "\n",
    "        return self._get_observation(), reward, done, False, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = [float(self.current_time)]\n",
    "\n",
    "        # Current job features\n",
    "        if self.current_job is not None:\n",
    "            obs.extend(\n",
    "                [\n",
    "                    float(self.current_job[\"Burst time\"]),\n",
    "                    float(self.current_job[\"Arrival Time\"]),\n",
    "                    float(self.current_job[\"Preemptive\"]),\n",
    "                    float(self.current_job[\"Resources\"]),\n",
    "                    float(self.current_time - self.current_job[\"Arrival Time\"]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            obs.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        # Waiting jobs features\n",
    "        for job in self.waiting_jobs:\n",
    "            obs.extend(\n",
    "                [\n",
    "                    float(job[\"Burst time\"]),\n",
    "                    float(job[\"Arrival Time\"]),\n",
    "                    float(job[\"Preemptive\"]),\n",
    "                    float(job[\"Resources\"]),\n",
    "                    float(self.current_time - job[\"Arrival Time\"]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Pad with zeros if there are fewer waiting jobs than total jobs\n",
    "        obs.extend([0.0] * 5 * (self.num_jobs - len(self.waiting_jobs)))\n",
    "\n",
    "        return np.array(obs, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard_logs/ppo_cpu_scheduler_4\n",
      "Eval num_timesteps=500, episode_reward=-160735.97 +/- 321387.55\n",
      "Episode length: 143.20 +/- 270.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 143       |\n",
      "|    mean_reward     | -1.61e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 500       |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-56.90 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -56.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-124.99 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-221.34 +/- 15.48\n",
      "Episode length: 14.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -221     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.93     |\n",
      "|    ep_rew_mean     | -74.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 370      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 8             |\n",
      "|    mean_reward          | -42.2         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8338127e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.2          |\n",
      "|    explained_variance   | 2.32e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.84e+07      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000656     |\n",
      "|    value_loss           | 1.75e+08      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.08     |\n",
      "|    ep_rew_mean     | -80.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 326      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035511628 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 0.0327      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.53e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 2.21e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.6      |\n",
      "|    ep_rew_mean     | -67.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 316      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020432314 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 623         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 1.26e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.32     |\n",
      "|    ep_rew_mean     | -52.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 304      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026964054 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 532         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.26     |\n",
      "|    ep_rew_mean     | -54.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01774015 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.08      |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 354        |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    value_loss           | 473        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23     |\n",
      "|    ep_rew_mean     | -53.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037991434 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 97.1        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.15     |\n",
      "|    ep_rew_mean     | -49.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01544538 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.01      |\n",
      "|    explained_variance   | 0.733      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 41.6       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    value_loss           | 115        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -46.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 255      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01638269 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.98      |\n",
      "|    explained_variance   | 0.775      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 38.6       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 88         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -43.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 250      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023606885 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.407       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 251      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019161562 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00991    |\n",
      "|    value_loss           | 7.98        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 248      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-5312596.92 +/- 5715.06\n",
      "Episode length: 2178.80 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.18e+03   |\n",
      "|    mean_reward          | -5.31e+06  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14025873 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.79      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-5448466.81 +/- 4992.49\n",
      "Episode length: 2206.40 +/- 1.02\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.21e+03  |\n",
      "|    mean_reward     | -5.45e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 23500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-5592924.95 +/- 5240.58\n",
      "Episode length: 2235.40 +/- 1.02\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.24e+03  |\n",
      "|    mean_reward     | -5.59e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 24000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-5731408.53 +/- 5620.26\n",
      "Episode length: 2262.80 +/- 1.17\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.26e+03  |\n",
      "|    mean_reward     | -5.73e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 24500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.3     |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 114      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-5882020.14 +/- 5694.19\n",
      "Episode length: 2292.20 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.29e+03     |\n",
      "|    mean_reward          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051399716 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.0477      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+03     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 1.25e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-6048273.76 +/- 6148.47\n",
      "Episode length: 2324.20 +/- 1.17\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.32e+03  |\n",
      "|    mean_reward     | -6.05e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 25500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-6219334.89 +/- 6011.75\n",
      "Episode length: 2356.80 +/- 1.17\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.36e+03  |\n",
      "|    mean_reward     | -6.22e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 26000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-6398224.04 +/- 5489.34\n",
      "Episode length: 2390.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.39e+03 |\n",
      "|    mean_reward     | -6.4e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.7     |\n",
      "|    ep_rew_mean     | -127     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-5391509.23 +/- 5095.01\n",
      "Episode length: 2251.60 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.25e+03    |\n",
      "|    mean_reward          | -5.39e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009098547 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.0032      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.13e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 8.05e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-5554926.09 +/- 5172.41\n",
      "Episode length: 2285.40 +/- 1.02\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.29e+03  |\n",
      "|    mean_reward     | -5.55e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 27500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-5721708.47 +/- 5108.94\n",
      "Episode length: 2319.40 +/- 1.02\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.32e+03  |\n",
      "|    mean_reward     | -5.72e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 28000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=-5883893.80 +/- 4752.23\n",
      "Episode length: 2352.00 +/- 0.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.35e+03  |\n",
      "|    mean_reward     | -5.88e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 28500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.6     |\n",
      "|    ep_rew_mean     | -96.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 68       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 415      |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-1129750.87 +/- 1041.31\n",
      "Episode length: 1067.60 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.07e+03    |\n",
      "|    mean_reward          | -1.13e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020853795 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.0309      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.84e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 2.69e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=-1167042.10 +/- 1366.46\n",
      "Episode length: 1085.00 +/- 0.63\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.08e+03  |\n",
      "|    mean_reward     | -1.17e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 29500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1204060.71 +/- 1387.96\n",
      "Episode length: 1102.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.1e+03  |\n",
      "|    mean_reward     | -1.2e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-1242548.74 +/- 1092.26\n",
      "Episode length: 1119.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.12e+03  |\n",
      "|    mean_reward     | -1.24e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 30500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.99     |\n",
      "|    ep_rew_mean     | -67.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 62       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-527517.69 +/- 0.00\n",
      "Episode length: 731.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 731          |\n",
      "|    mean_reward          | -5.28e+05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098784175 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 253          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    value_loss           | 958          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-544797.36 +/- 590.23\n",
      "Episode length: 742.80 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 743       |\n",
      "|    mean_reward     | -5.45e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 31500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-562355.43 +/- 734.63\n",
      "Episode length: 754.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 755       |\n",
      "|    mean_reward     | -5.62e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 32000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-580191.90 +/- 746.39\n",
      "Episode length: 766.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 766      |\n",
      "|    mean_reward     | -5.8e+05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.2      |\n",
      "|    ep_rew_mean     | -55.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 549      |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-251717.00 +/- 491.64\n",
      "Episode length: 506.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 506         |\n",
      "|    mean_reward          | -2.52e+05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011771771 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 74.4        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 207         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-259807.92 +/- 499.48\n",
      "Episode length: 514.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 514      |\n",
      "|    mean_reward     | -2.6e+05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-268026.84 +/- 507.32\n",
      "Episode length: 522.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 522       |\n",
      "|    mean_reward     | -2.68e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 34000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-276373.76 +/- 515.16\n",
      "Episode length: 530.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 530       |\n",
      "|    mean_reward     | -2.76e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 34500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.69     |\n",
      "|    ep_rew_mean     | -48.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-69876.66 +/- 0.00\n",
      "Episode length: 269.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 269         |\n",
      "|    mean_reward          | -6.99e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018081903 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.3        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 73.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-72329.46 +/- 263.35\n",
      "Episode length: 273.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 274       |\n",
      "|    mean_reward     | -7.23e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 35500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-74714.75 +/- 0.00\n",
      "Episode length: 278.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 278       |\n",
      "|    mean_reward     | -7.47e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 36000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-76917.01 +/- 0.00\n",
      "Episode length: 282.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 282       |\n",
      "|    mean_reward     | -7.69e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 36500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.24     |\n",
      "|    ep_rew_mean     | -44.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 617      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028523099 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.11     |\n",
      "|    ep_rew_mean     | -43.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 61       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 634      |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017727561 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 62       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 652      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021166313 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -43.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012224736 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0406     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 8.31        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -42.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 683      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019910268 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.73        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00999    |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 68       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 691      |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015135266 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 70       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 699      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 49500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915125 |\n",
      "|    clip_fraction        | 0.0805     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.77      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.348      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00634   |\n",
      "|    value_loss           | 5.31       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 710      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8            |\n",
      "|    mean_reward          | -42.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 51500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131212855 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00746     |\n",
      "|    value_loss           | 4            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 729      |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-39908929.39 +/- 18214.25\n",
      "Episode length: 4595.60 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6e+03    |\n",
      "|    mean_reward          | -3.99e+07  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 53500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17176786 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.69      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0787    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-40394786.40 +/- 16697.63\n",
      "Episode length: 4623.40 +/- 1.02\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 4.62e+03  |\n",
      "|    mean_reward     | -4.04e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 54000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-40916129.46 +/- 14506.40\n",
      "Episode length: 4653.00 +/- 0.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 4.65e+03  |\n",
      "|    mean_reward     | -4.09e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 54500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-41404116.87 +/- 16272.04\n",
      "Episode length: 4680.00 +/- 0.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 4.68e+03  |\n",
      "|    mean_reward     | -4.14e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 55000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14       |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 949      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-39519174.10 +/- 15903.54\n",
      "Episode length: 4450.00 +/- 0.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.45e+03    |\n",
      "|    mean_reward          | -3.95e+07   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005026362 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.00169     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.83e+03    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 9.04e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-40029338.44 +/- 18248.93\n",
      "Episode length: 4478.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.48e+03 |\n",
      "|    mean_reward     | -4e+07   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-40557182.55 +/- 16111.05\n",
      "Episode length: 4508.00 +/- 0.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 4.51e+03  |\n",
      "|    mean_reward     | -4.06e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 56500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-41117498.17 +/- 16221.96\n",
      "Episode length: 4539.00 +/- 0.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 4.54e+03  |\n",
      "|    mean_reward     | -4.11e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 57000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.1     |\n",
      "|    ep_rew_mean     | -119     |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 1146     |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=-6702027.61 +/- 3587.37\n",
      "Episode length: 1835.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.84e+03    |\n",
      "|    mean_reward          | -6.7e+06    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010555847 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0368      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.78e+03    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 5.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-6800505.36 +/- 2949.88\n",
      "Episode length: 1848.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.85e+03 |\n",
      "|    mean_reward     | -6.8e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-6898216.17 +/- 4698.32\n",
      "Episode length: 1862.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.86e+03 |\n",
      "|    mean_reward     | -6.9e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-6998119.51 +/- 3665.76\n",
      "Episode length: 1875.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.88e+03 |\n",
      "|    mean_reward     | -7e+06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11       |\n",
      "|    ep_rew_mean     | -89.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 1249     |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=-3706007.60 +/- 1721.05\n",
      "Episode length: 1429.80 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.43e+03     |\n",
      "|    mean_reward          | -3.71e+06    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 59500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072630253 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+03     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0393      |\n",
      "|    value_loss           | 2.13e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-3762369.60 +/- 1406.77\n",
      "Episode length: 1440.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.44e+03  |\n",
      "|    mean_reward     | -3.76e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 60000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-3819157.20 +/- 2080.87\n",
      "Episode length: 1451.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.45e+03  |\n",
      "|    mean_reward     | -3.82e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 60500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-3878595.08 +/- 1428.33\n",
      "Episode length: 1462.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.46e+03  |\n",
      "|    mean_reward     | -3.88e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 61000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.1     |\n",
      "|    ep_rew_mean     | -69.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 1344     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-996127.89 +/- 927.65\n",
      "Episode length: 951.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 951         |\n",
      "|    mean_reward          | -9.96e+05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011473771 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 481         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 768         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-1012480.39 +/- 0.00\n",
      "Episode length: 959.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 959       |\n",
      "|    mean_reward     | -1.01e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 62000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=-1027939.03 +/- 942.35\n",
      "Episode length: 966.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 966       |\n",
      "|    mean_reward     | -1.03e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 62500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-1044549.52 +/- 0.00\n",
      "Episode length: 974.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 974       |\n",
      "|    mean_reward     | -1.04e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 63000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.25     |\n",
      "|    ep_rew_mean     | -58.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 1406     |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=-903828.88 +/- 931.57\n",
      "Episode length: 955.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 955         |\n",
      "|    mean_reward          | -9.04e+05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 63500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020095427 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.7        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 339         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-918720.29 +/- 767.03\n",
      "Episode length: 963.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 963       |\n",
      "|    mean_reward     | -9.19e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 64000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-933733.30 +/- 0.00\n",
      "Episode length: 971.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 971       |\n",
      "|    mean_reward     | -9.34e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 64500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-949257.81 +/- 0.00\n",
      "Episode length: 979.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 979       |\n",
      "|    mean_reward     | -9.49e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 65000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-965303.45 +/- 786.23\n",
      "Episode length: 987.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 987       |\n",
      "|    mean_reward     | -9.65e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 65500     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.56     |\n",
      "|    ep_rew_mean     | -48.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 1486     |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028761815 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 72          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.27     |\n",
      "|    ep_rew_mean     | -45      |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 1501     |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009471784 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.16     |\n",
      "|    ep_rew_mean     | -43.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 1517     |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01353001 |\n",
      "|    clip_fraction        | 0.0795     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.76      |\n",
      "|    explained_variance   | 0.947      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.9       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    value_loss           | 13.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -42.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 1532     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010917917 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 4.86        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 1548     |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010375153 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00944    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 1567     |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013246224 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1586     |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 78000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01637713 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.11       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    value_loss           | 1.62       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 1603     |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028925445 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.518       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1621     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 82000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01184818 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.423      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 2.08       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 1638     |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018283369 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.567       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1654     |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=-953549.66 +/- 781.43\n",
      "Episode length: 981.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 981         |\n",
      "|    mean_reward          | -9.54e+05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 86500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021439308 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 6.53        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-964910.33 +/- 0.00\n",
      "Episode length: 987.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 987       |\n",
      "|    mean_reward     | -9.65e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 87000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=-976733.72 +/- 0.00\n",
      "Episode length: 993.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 993       |\n",
      "|    mean_reward     | -9.77e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 87500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-987834.09 +/- 973.70\n",
      "Episode length: 998.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -9.88e+05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 88000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.72     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 1730     |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016177144 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.6        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.29     |\n",
      "|    ep_rew_mean     | -44.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1746     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02881218 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.65      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.44       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    value_loss           | 14.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.05     |\n",
      "|    ep_rew_mean     | -42.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 1762     |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014338873 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1777     |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 94500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016525835 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.254       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00962    |\n",
      "|    value_loss           | 0.465       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1795     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019957399 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.373       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.456       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 54       |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1813     |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=-1028800.42 +/- 0.00\n",
      "Episode length: 1019.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.02e+03    |\n",
      "|    mean_reward          | -1.03e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 98500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024102602 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.000217    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-1039784.07 +/- 999.18\n",
      "Episode length: 1024.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.02e+03  |\n",
      "|    mean_reward     | -1.04e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 99000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=-1051235.63 +/- 0.00\n",
      "Episode length: 1030.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.03e+03  |\n",
      "|    mean_reward     | -1.05e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 99500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-1061513.46 +/- 0.00\n",
      "Episode length: 1035.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.04e+03  |\n",
      "|    mean_reward     | -1.06e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 100000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.34     |\n",
      "|    ep_rew_mean     | -45      |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 1884     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023455119 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 37.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -42.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1897     |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018857611 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 54       |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 1912     |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 104500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01627851 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.66      |\n",
      "|    explained_variance   | 0.998      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    value_loss           | 0.465      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1929     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 106500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026708089 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00703     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 1947     |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 109000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01989987 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.63      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.00903   |\n",
      "|    value_loss           | 0.000143   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 56       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1963     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-3194410.16 +/- 0.00\n",
      "Episode length: 1792.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.79e+03  |\n",
      "|    mean_reward          | -3.19e+06 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 111000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0343346 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.62     |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0201   |\n",
      "|    n_updates            | 540       |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    value_loss           | 2.11e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=-3223788.99 +/- 1436.63\n",
      "Episode length: 1800.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.8e+03   |\n",
      "|    mean_reward     | -3.22e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 111500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-3251859.20 +/- 0.00\n",
      "Episode length: 1808.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.81e+03  |\n",
      "|    mean_reward     | -3.25e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 112000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-3280775.72 +/- 0.00\n",
      "Episode length: 1816.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.82e+03  |\n",
      "|    mean_reward     | -3.28e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 112500    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.01     |\n",
      "|    ep_rew_mean     | -52      |\n",
      "| time/              |          |\n",
      "|    fps             | 54       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 2056     |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-3150949.47 +/- 1419.83\n",
      "Episode length: 1779.80 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.78e+03    |\n",
      "|    mean_reward          | -3.15e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 113000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010295062 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 203         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=-3180127.90 +/- 0.00\n",
      "Episode length: 1788.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.79e+03  |\n",
      "|    mean_reward     | -3.18e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 113500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-3208724.42 +/- 0.00\n",
      "Episode length: 1796.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.8e+03   |\n",
      "|    mean_reward     | -3.21e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 114000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=-3237448.94 +/- 0.00\n",
      "Episode length: 1804.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.8e+03   |\n",
      "|    mean_reward     | -3.24e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 114500    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.57     |\n",
      "|    ep_rew_mean     | -47      |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 2157     |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902773 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12         |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 32         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.21     |\n",
      "|    ep_rew_mean     | -43.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 2167     |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029274864 |\n",
      "|    clip_fraction        | 0.0575      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 7.07        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 54       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 2178     |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013719067 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00295    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    value_loss           | 0.689       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -42.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 2192     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 121000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010557247 |\n",
      "|    clip_fraction        | 0.0714      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 2206     |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015096126 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.417       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    value_loss           | 0.454       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 56       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 2219     |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011941399 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    value_loss           | 0.668       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 56       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 2231     |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042734466 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.085      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.000129    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -42.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 57       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 2243     |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 129500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017194211 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.89        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 7.55        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 2255     |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 131500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016032748 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.481       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.922       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 2269     |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 133500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015412394 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00384     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 2283     |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023638822 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    value_loss           | 7.33e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 2294     |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 137500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02119461 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.64      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000829   |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 5.42e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 2306     |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 139500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015112612 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.764       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    value_loss           | 0.669       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 2318     |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=-5081821.01 +/- 0.00\n",
      "Episode length: 2259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.26e+03    |\n",
      "|    mean_reward          | -5.08e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025695367 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.000127    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-5116144.11 +/- 2216.09\n",
      "Episode length: 2266.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.27e+03  |\n",
      "|    mean_reward     | -5.12e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 142000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=-5152398.23 +/- 2223.92\n",
      "Episode length: 2274.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.27e+03  |\n",
      "|    mean_reward     | -5.15e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 142500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-5187869.24 +/- 2231.76\n",
      "Episode length: 2282.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.28e+03  |\n",
      "|    mean_reward     | -5.19e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 143000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.71     |\n",
      "|    ep_rew_mean     | -49      |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 2420     |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 143500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017964732 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.7        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 73.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23     |\n",
      "|    ep_rew_mean     | -44.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 2432     |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 145500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03702223 |\n",
      "|    clip_fraction        | 0.0939     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.947      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.8        |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    value_loss           | 13.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -43.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 2444     |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 147500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012233175 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00987    |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 2457     |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016909659 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 61       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 2469     |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017122645 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.375       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 61       |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 2478     |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 154000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011946702 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 62       |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 2486     |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028954705 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00931    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 2494     |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 158000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016023856 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0436     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00688    |\n",
      "|    value_loss           | 1.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 2501     |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020386908 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    value_loss           | 2.2e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 2509     |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 162000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016761303 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 5.14e-11    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 2517     |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 164000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02184356 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00701   |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.00965   |\n",
      "|    value_loss           | 7.63e-13   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 2526     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 166000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016022488 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    value_loss           | 3.11e-13    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.07     |\n",
      "|    ep_rew_mean     | -42.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 2534     |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013165968 |\n",
      "|    clip_fraction        | 0.0605      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.216       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -42.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 2543     |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015758125 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0083     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 67       |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 2552     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 172500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017236877 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00999    |\n",
      "|    value_loss           | 0.818       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 67       |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 2563     |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=-8244294.62 +/- 0.00\n",
      "Episode length: 2876.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.88e+03    |\n",
      "|    mean_reward          | -8.24e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056838967 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 7.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-8287996.11 +/- 2820.62\n",
      "Episode length: 2883.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.88e+03  |\n",
      "|    mean_reward     | -8.29e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 175000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=-8330658.10 +/- 0.00\n",
      "Episode length: 2891.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.89e+03  |\n",
      "|    mean_reward     | -8.33e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 175500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-8372272.56 +/- 2315.03\n",
      "Episode length: 2898.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.9e+03   |\n",
      "|    mean_reward     | -8.37e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 176000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.48     |\n",
      "|    ep_rew_mean     | -57      |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 2655     |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=-8061556.54 +/- 0.00\n",
      "Episode length: 2844.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.84e+03    |\n",
      "|    mean_reward          | -8.06e+06   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007723459 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 330         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-8104771.63 +/- 2789.27\n",
      "Episode length: 2851.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.85e+03 |\n",
      "|    mean_reward     | -8.1e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=-8148101.93 +/- 2283.83\n",
      "Episode length: 2859.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.86e+03  |\n",
      "|    mean_reward     | -8.15e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 177500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-8192692.53 +/- 0.00\n",
      "Episode length: 2867.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.87e+03  |\n",
      "|    mean_reward     | -8.19e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 178000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.81     |\n",
      "|    ep_rew_mean     | -49.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 2742     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=-4366804.62 +/- 2047.56\n",
      "Episode length: 2094.40 +/- 0.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.09e+03     |\n",
      "|    mean_reward          | -4.37e+06    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 178500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058518294 |\n",
      "|    clip_fraction        | 0.186        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 68.6         |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.0364      |\n",
      "|    value_loss           | 186          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-4390240.18 +/- 0.00\n",
      "Episode length: 2100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.1e+03   |\n",
      "|    mean_reward     | -4.39e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 179000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=-4415419.57 +/- 0.00\n",
      "Episode length: 2106.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.11e+03  |\n",
      "|    mean_reward     | -4.42e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 179500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-4440670.96 +/- 0.00\n",
      "Episode length: 2112.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 2.11e+03  |\n",
      "|    mean_reward     | -4.44e+06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 180000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.49     |\n",
      "|    ep_rew_mean     | -46.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 2810     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014370399 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 53.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.21     |\n",
      "|    ep_rew_mean     | -43.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 2823     |\n",
      "|    total_timesteps | 182272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 182500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029173166 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.84        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 2837     |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012961493 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00951    |\n",
      "|    value_loss           | 4.62        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 2851     |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 186500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013474455 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    value_loss           | 0.462       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 2864     |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022849519 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.024       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.303       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 2876     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -42.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 190500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03142278 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0522    |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 3.24e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -42.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 2889     |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8            |\n",
      "|    mean_reward          | -42.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 193000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125010675 |\n",
      "|    clip_fraction        | 0.0964       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.288        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    value_loss           | 0.454        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 2904     |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015077157 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0702     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 3.21e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 67       |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -42.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 197000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023016019 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0925      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 4.39e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=-42.20 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -42.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 67       |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 2940     |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-10783915.94 +/- 3217.44\n",
      "Episode length: 3288.60 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.29e+03    |\n",
      "|    mean_reward          | -1.08e+07   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030320931 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0619     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 7.19e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=-10836522.06 +/- 3225.27\n",
      "Episode length: 3296.60 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 3.3e+03   |\n",
      "|    mean_reward     | -1.08e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 199500    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-10886616.35 +/- 2639.83\n",
      "Episode length: 3304.20 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 3.3e+03   |\n",
      "|    mean_reward     | -1.09e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 200000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=-10940795.19 +/- 3240.95\n",
      "Episode length: 3312.40 +/- 0.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 3.31e+03  |\n",
      "|    mean_reward     | -1.09e+07 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 200500    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.75     |\n",
      "|    ep_rew_mean     | -48.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 3098     |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Mean reward: -5568605.12 +/- 2359.78\n",
      "Final evaluation over 401 episodes:\n",
      "Mean reward: -10940795.19\n",
      "Std of reward: 3240.95\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback,\n",
    "    StopTrainingOnRewardThreshold,\n",
    ")\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming CPUSchedulerEnv is defined as in the previous artifact\n",
    "jobs_data = [\n",
    "    {\n",
    "        \"Job Id\": 247,\n",
    "        \"Burst time\": 199,\n",
    "        \"Arrival Time\": 0.41,\n",
    "        \"Preemptive\": 0,\n",
    "        \"Resources\": 8,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 29,\n",
    "        \"Burst time\": 193,\n",
    "        \"Arrival Time\": 0.5925,\n",
    "        \"Preemptive\": 1,\n",
    "        \"Resources\": 2,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 170,\n",
    "        \"Burst time\": 75,\n",
    "        \"Arrival Time\": 0.36,\n",
    "        \"Preemptive\": 1,\n",
    "        \"Resources\": 4,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 164,\n",
    "        \"Burst time\": 42,\n",
    "        \"Arrival Time\": 0.9725,\n",
    "        \"Preemptive\": 0,\n",
    "        \"Resources\": 8,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 312,\n",
    "        \"Burst time\": 257,\n",
    "        \"Arrival Time\": 0.6125,\n",
    "        \"Preemptive\": 0,\n",
    "        \"Resources\": 4,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 36,\n",
    "        \"Burst time\": 131,\n",
    "        \"Arrival Time\": 0.6775,\n",
    "        \"Preemptive\": 0,\n",
    "        \"Resources\": 8,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 99,\n",
    "        \"Burst time\": 192,\n",
    "        \"Arrival Time\": 0.8675,\n",
    "        \"Preemptive\": 1,\n",
    "        \"Resources\": 4,\n",
    "    },\n",
    "    {\n",
    "        \"Job Id\": 30,\n",
    "        \"Burst time\": 92,\n",
    "        \"Arrival Time\": 0.435,\n",
    "        \"Preemptive\": 1,\n",
    "        \"Resources\": 8,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create and wrap the environment\n",
    "# env = DummyVecEnv([lambda: CPUSchedulerEnv(jobs_data)])\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(CPUSchedulerEnv(jobs_data))])\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: Monitor(CPUSchedulerEnv(jobs_data))])\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=\"./tensorboard_logs/\")\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=500,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    callback_on_new_best=None,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Initialize the agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "total_timesteps = 200000\n",
    "# model.learn(total_timesteps=total_timesteps)\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=eval_callback,\n",
    "    tb_log_name=\"ppo_cpu_scheduler\",\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_cpu_scheduler\")\n",
    "\n",
    "# Plotting training progress\n",
    "results = np.load(\"./logs/evaluations.npz\")\n",
    "x = results[\"timesteps\"]\n",
    "y = results[\"results\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Training Progress\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.savefig(\"training_progress.png\")\n",
    "plt.close()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final evaluation over {len(y)} episodes:\")\n",
    "print(f\"Mean reward: {y[-1].mean():.2f}\")\n",
    "print(f\"Std of reward: {y[-1].std():.2f}\")\n",
    "\n",
    "tb_writer.close()\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = PPO.load(\"ppo_cpu_scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "def evaluate_scheduler(env, model, n_episodes=100):\n",
    "    total_reward = 0\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_reward += episode_reward\n",
    "    return total_reward / n_episodes\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_cpu_scheduler\")\n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env = CPUSchedulerEnv(jobs_data)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_reward = evaluate_scheduler(eval_env, model)\n",
    "print(f\"Average reward over 100 episodes: {avg_reward:.2f}\")\n",
    "\n",
    "# You can also implement and evaluate traditional scheduling algorithms here\n",
    "# for comparison, such as FCFS, SJF, or Round Robin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
